---
phase: 29-production-data-migration
plan: 03
type: execute
wave: 3
depends_on: ['29-02']
files_modified: []
autonomous: false

must_haves:
  truths:
    - 'Import completes without errors'
    - 'Row counts validated for critical tables'
    - 'Migration logs reviewed with no unresolved errors'
  artifacts: []
  key_links: []
---

<objective>
Import exported data into the new PostgreSQL database and validate integrity.

Purpose: Load the snapshot into the target system and confirm core data correctness.
Output: Target database populated and validated for cutover.
</objective>

<context>
@.planning/phases/29-production-data-migration/29-RESEARCH.md
@scripts/migration/README.md
</context>

<tasks>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Checkpoint 1: Import production data</name>
  <what-built>New database populated from migration-data.</what-built>
  <how-to-verify>
```bash
cd scripts/migration
pnpm run import -- --disable-fk
```
- Confirm import completes without fatal errors
- If import fails, use --truncate-first --yes and retry
  </how-to-verify>
  <resume-signal>Type "imported" when import completes successfully</resume-signal>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Checkpoint 2: Validate migration integrity</name>
  <what-built>Critical data checks pass before cutover.</what-built>
  <how-to-verify>
1. Review migration log for errors:
   ```bash
   cd scripts/migration
   rg -n "error|failed" -i migration.log
   ```
2. Validate row counts for critical tables (run in both source and target):
   - organizations
   - sites
   - areas
   - units
   - readings
   - alerts
3. Confirm user IDs in profiles reference Stack Auth IDs (spot check).
  </how-to-verify>
  <resume-signal>Type "validated" when integrity checks pass</resume-signal>
</task>

</tasks>
