---
phase: 15-background-jobs-infrastructure
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/workers/index.ts
  - backend/Dockerfile.worker
  - docker-compose.yml
  - docker-compose.prod.yml
autonomous: true

must_haves:
  truths:
    - 'Worker process starts independently from API server'
    - 'Worker connects to Redis with maxRetriesPerRequest: null'
    - 'Worker gracefully shuts down on SIGTERM/SIGINT'
    - 'Worker container can be deployed separately from API container'
  artifacts:
    - path: 'backend/src/workers/index.ts'
      provides: 'Worker entry point with processor stubs'
      contains: 'new Worker'
    - path: 'backend/Dockerfile.worker'
      provides: 'Multi-stage Docker build for worker container'
      contains: 'node workers/index.js'
    - path: 'docker-compose.yml'
      provides: 'Worker service definition'
      contains: 'frostguard-worker'
  key_links:
    - from: 'backend/src/workers/index.ts'
      to: 'backend/src/jobs/index.ts'
      via: 'imports QueueNames for queue registration'
      pattern: 'import.*QueueNames'
    - from: 'docker-compose.yml'
      to: 'backend/Dockerfile.worker'
      via: 'build context and dockerfile reference'
      pattern: 'dockerfile.*Dockerfile.worker'
---

<objective>
Create worker container entry point for independent job processing deployment.

Purpose: Enables horizontal scaling of job processing separately from API servers. Workers can be scaled based on job queue depth while API instances scale based on request load.

Output: Worker entry point with graceful shutdown, Dockerfile for worker container, docker-compose service definition.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-background-jobs-infrastructure/15-RESEARCH.md

# Existing patterns

@docker-compose.yml
@backend/Dockerfile
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create worker entry point with processor stubs</name>
  <files>backend/src/workers/index.ts</files>
  <action>
Create `backend/src/workers/index.ts` as the worker container entry point:

```typescript
/**
 * Worker entry point for background job processing
 *
 * This file is the entry point for worker containers, separate from the API server.
 * Workers process jobs from BullMQ queues and can be scaled independently.
 *
 * CRITICAL: Workers MUST use maxRetriesPerRequest: null for Redis connection
 * to handle blocking operations correctly.
 *
 * Usage:
 *   node dist/workers/index.js
 *   # or in development:
 *   tsx src/workers/index.ts
 */

import { Worker } from 'bullmq';
import IORedis from 'ioredis';
import { QueueNames } from '../jobs/index.js';

// Load environment variables
import 'dotenv/config';

console.log('[Worker] Starting background job worker...');

// Redis configuration
const redisUrl = process.env.REDIS_URL;
const redisHost = process.env.REDIS_HOST || 'localhost';
const redisPort = parseInt(process.env.REDIS_PORT || '6379', 10);

if (!redisUrl && !process.env.REDIS_HOST) {
  console.error('[Worker] FATAL: Redis configuration required. Set REDIS_URL or REDIS_HOST.');
  process.exit(1);
}

// CRITICAL: Workers MUST use maxRetriesPerRequest: null
// This is required for BullMQ's blocking Redis operations (BRPOPLPUSH)
const connection = redisUrl
  ? new IORedis(redisUrl, {
      maxRetriesPerRequest: null,
      enableReadyCheck: false,
    })
  : new IORedis({
      host: redisHost,
      port: redisPort,
      maxRetriesPerRequest: null,
      enableReadyCheck: false,
    });

connection.on('error', (err) => {
  console.error('[Worker] Redis connection error:', err.message);
});

connection.on('connect', () => {
  console.log('[Worker] Redis connected');
});

// Track active workers for graceful shutdown
const workers: Worker[] = [];

/**
 * SMS Notification Worker
 *
 * Processes SMS notification jobs. Actual implementation in Phase 16.
 * This is a stub that logs job data for verification.
 */
const smsWorker = new Worker(
  QueueNames.SMS_NOTIFICATIONS,
  async (job) => {
    console.log(`[Worker:SMS] Processing job ${job.id}:`, {
      name: job.name,
      organizationId: job.data.organizationId,
      // Don't log sensitive data like phone numbers in production
    });

    // TODO: Phase 16 - Implement actual SMS sending via Telnyx
    // For now, just simulate processing time
    await new Promise((resolve) => setTimeout(resolve, 100));

    console.log(`[Worker:SMS] Job ${job.id} completed (stub)`);
    return { success: true, stub: true };
  },
  {
    connection,
    concurrency: 5, // Process 5 SMS jobs concurrently
  },
);

workers.push(smsWorker);

/**
 * Email Digest Worker
 *
 * Processes email digest jobs. Actual implementation in Phase 17.
 * This is a stub that logs job data for verification.
 */
const emailWorker = new Worker(
  QueueNames.EMAIL_DIGESTS,
  async (job) => {
    console.log(`[Worker:Email] Processing job ${job.id}:`, {
      name: job.name,
      organizationId: job.data.organizationId,
      period: job.data.period,
    });

    // TODO: Phase 17 - Implement actual email digest generation
    // For now, just simulate processing time
    await new Promise((resolve) => setTimeout(resolve, 200));

    console.log(`[Worker:Email] Job ${job.id} completed (stub)`);
    return { success: true, stub: true };
  },
  {
    connection,
    concurrency: 2, // Emails are slower, limit concurrency
  },
);

workers.push(emailWorker);

// Worker event handlers
workers.forEach((worker) => {
  worker.on('completed', (job) => {
    console.log(`[Worker] ✓ Job ${job.id} completed in queue ${job.queueName}`);
  });

  worker.on('failed', (job, err) => {
    console.error(`[Worker] ✗ Job ${job?.id} failed in queue ${job?.queueName}:`, err.message);
  });

  worker.on('error', (err) => {
    console.error('[Worker] Worker error:', err);
  });

  worker.on('stalled', (jobId) => {
    console.warn(`[Worker] Job ${jobId} stalled`);
  });
});

/**
 * Graceful shutdown handler
 *
 * Closes all workers and Redis connection when process receives
 * SIGTERM (Docker stop) or SIGINT (Ctrl+C).
 */
const gracefulShutdown = async (signal: string) => {
  console.log(`[Worker] ${signal} received, shutting down gracefully...`);

  // Close all workers - this waits for active jobs to complete
  const closePromises = workers.map((worker) =>
    worker.close().catch((err) => {
      console.error('[Worker] Error closing worker:', err.message);
    }),
  );

  await Promise.all(closePromises);
  console.log('[Worker] All workers closed');

  // Close Redis connection
  await connection.quit().catch(() => {});
  console.log('[Worker] Redis connection closed');

  process.exit(0);
};

process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
process.on('SIGINT', () => gracefulShutdown('SIGINT'));

console.log('[Worker] Workers started and ready to process jobs');
console.log(`[Worker] - ${QueueNames.SMS_NOTIFICATIONS}: concurrency=5`);
console.log(`[Worker] - ${QueueNames.EMAIL_DIGESTS}: concurrency=2`);
```

  </action>
  <verify>
    - TypeScript compiles: `cd backend && npx tsc --noEmit`
    - File imports from jobs/index.ts correctly
    - Contains maxRetriesPerRequest: null in Redis config
    - Has SIGTERM and SIGINT handlers
  </verify>
  <done>
    Worker entry point created with SMS and email worker stubs, graceful shutdown handling, and proper Redis configuration.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Dockerfile.worker for worker container</name>
  <files>backend/Dockerfile.worker</files>
  <action>
Create `backend/Dockerfile.worker` for building worker containers:

```dockerfile
# ============================================
# FreshTrack Pro - Worker Container
# ============================================
# Multi-stage build for minimal production image
# Runs BullMQ workers for background job processing

# Stage 1: Builder
FROM node:20-alpine AS builder

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install all dependencies (including dev for build)
RUN npm ci

# Copy source code
COPY . .

# Build TypeScript
RUN npm run build

# Stage 2: Production
FROM node:20-alpine AS production

WORKDIR /app

# Create non-root user for security
RUN addgroup -g 1001 -S worker && \
    adduser -S worker -u 1001 -G worker

# Copy package files
COPY package*.json ./

# Install only production dependencies
RUN npm ci --only=production && \
    npm cache clean --force

# Copy built worker files (only what's needed)
COPY --from=builder /app/dist/workers ./workers
COPY --from=builder /app/dist/jobs ./jobs

# Set ownership to non-root user
RUN chown -R worker:worker /app

USER worker

# Health check - workers don't have HTTP endpoint, so check process
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD pgrep -x node || exit 1

# Run worker entry point
CMD ["node", "workers/index.js"]
```

  </action>
  <verify>
    - Dockerfile exists at backend/Dockerfile.worker
    - Contains multi-stage build (builder, production stages)
    - Final CMD runs workers/index.js
    - Uses non-root user for security
  </verify>
  <done>
    Dockerfile.worker created with multi-stage build, non-root user, and health check.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add worker service to docker-compose files</name>
  <files>
    docker-compose.yml
    docker-compose.prod.yml
  </files>
  <action>
1. Add worker service to `docker-compose.yml` (development):

Add after the backend service definition, before redis:

```yaml
# ===========================================
# Worker (Background Job Processing)
# ===========================================
worker:
  build:
    context: ./backend
    dockerfile: Dockerfile.worker
  container_name: frostguard-worker
  environment:
    - NODE_ENV=development
    - REDIS_HOST=redis
    - REDIS_PORT=6379
  depends_on:
    redis:
      condition: service_healthy
  restart: unless-stopped
  networks:
    - frostguard-network
```

2. Add worker service to `docker-compose.prod.yml` (production overlay):

   Add to the services section:

   ```yaml
   worker:
     build:
       context: ./backend
       dockerfile: Dockerfile.worker
     container_name: frostguard-worker
     environment:
       - NODE_ENV=production
     env_file:
       - .env
     deploy:
       resources:
         limits:
           cpus: '0.5'
           memory: 256M
         reservations:
           cpus: '0.1'
           memory: 128M
     logging:
       driver: 'json-file'
       options:
         max-size: '10m'
         max-file: '3'
     restart: unless-stopped
   ```

   Note: The production worker reads REDIS_URL from .env file (via env_file).
   </action>
   <verify> - `docker compose config` validates without errors - `docker compose -f docker-compose.yml -f docker-compose.prod.yml config` validates - Worker service has correct depends_on for redis - Worker service has restart policy
   </verify>
   <done>
   Worker service added to both docker-compose files with appropriate environment configuration.
   </done>
   </task>

</tasks>

<verification>
1. TypeScript compiles: `cd backend && npm run build`
2. Docker build succeeds: `docker build -f backend/Dockerfile.worker -t freshtrack-worker ./backend`
3. Docker compose validates: `docker compose config`
4. Worker starts with docker-compose: `docker compose up worker redis -d` and check logs
5. Worker logs show "Workers started and ready to process jobs"
6. Graceful shutdown works: `docker compose stop worker` should show shutdown logs
</verification>

<success_criteria>

- Worker entry point at backend/src/workers/index.ts
- Dockerfile.worker with multi-stage build
- Worker service in docker-compose.yml with Redis dependency
- Worker service in docker-compose.prod.yml with resource limits
- Worker uses maxRetriesPerRequest: null for Redis
- Graceful shutdown on SIGTERM/SIGINT
  </success_criteria>

<output>
After completion, create `.planning/phases/15-background-jobs-infrastructure/15-02-SUMMARY.md`
</output>
