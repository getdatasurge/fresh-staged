---
phase: 13-e2e-validation-cutover
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/test/generate-test-data.ts
  - scripts/test/validate-migration-timing.sh
  - scripts/test/README.md
autonomous: true

must_haves:
  truths:
    - 'Synthetic data generator creates 100K sensor readings with realistic distribution'
    - 'Generated data maintains referential integrity (valid unit IDs, timestamps)'
    - 'Migration timing script measures pg_dump export duration'
    - 'Migration timing script measures pg_restore import duration'
    - 'Total migration window is documented for planning maintenance windows'
  artifacts:
    - path: 'scripts/test/generate-test-data.ts'
      provides: 'Synthetic sensor reading generator using Faker.js'
      min_lines: 80
    - path: 'scripts/test/validate-migration-timing.sh'
      provides: 'Migration timing validation script'
      min_lines: 100
  key_links:
    - from: 'generate-test-data.ts'
      to: 'backend/src/db/schema/sensor-readings.ts'
      via: 'Drizzle insert'
      pattern: "db\\.insert"
    - from: 'validate-migration-timing.sh'
      to: 'docker/docker-compose.yml'
      via: 'pg_dump/pg_restore via Docker'
      pattern: 'docker exec.*pg_dump'
---

<objective>
Create migration timing validation scripts that test data migration procedures with production-scale synthetic data.

Purpose: Validate TEST-03 requirement that migration procedure is tested with production-sized dataset and timing is documented.
Output: Scripts to generate 100K synthetic records and measure migration timing for maintenance window planning.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-e2e-validation-cutover/13-RESEARCH.md
@backend/src/db/schema/index.ts
@scripts/migration/README.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create synthetic data generator script</name>
  <files>scripts/test/generate-test-data.ts</files>
  <action>
Create scripts/test/generate-test-data.ts using Faker.js to generate production-scale test data:

1. **Setup:**
   - Import @faker-js/faker (install if needed: pnpm add -D @faker-js/faker in scripts/test)
   - Import database client from backend
   - Configure batch size (5000 records per batch for performance)

2. **Data generation parameters:**
   - TARGET_RECORDS: 100,000 (configurable via env var)
   - BATCH_SIZE: 5,000
   - NUM_DEVICES: 30 (simulating 20-50 sensors)
   - TIME_RANGE: 30 days of historical data

3. **Generate realistic sensor readings:**
   - device_id: sensor-001 through sensor-030 format
   - temperature: -20°C to 40°C (food safety range), float with 2 decimals
   - humidity: 0-100%, float with 1 decimal
   - recorded_at: Random timestamps over 30-day period
   - Distribute readings across devices (realistic workload)

4. **Data quality:**
   - Ensure timestamps are properly distributed (not all at same time)
   - Include some temperature excursions (5-10% above threshold) for realistic alert data
   - Use faker.number.float for realistic decimal values

5. **Progress tracking:**
   - Print progress every batch (e.g., "Batch 5/20 inserted (25,000 records)")
   - Print total time at completion
   - Print final row count

6. **Safety:**
   - Require confirmation before running (unless --yes flag)
   - Check if data already exists (offer to skip or truncate)
   - Use transaction for each batch (not entire operation - too large)

Script should be runnable with: npx tsx scripts/test/generate-test-data.ts
</action>
<verify>
Run: head -50 scripts/test/generate-test-data.ts
Script should have clear structure with faker imports and batch processing logic.
</verify>
<done>
generate-test-data.ts creates 100K realistic sensor readings.
Script includes progress tracking and confirmation prompts.
</done>
</task>

<task type="auto">
  <name>Task 2: Create migration timing validation script</name>
  <files>scripts/test/validate-migration-timing.sh</files>
  <action>
Create scripts/test/validate-migration-timing.sh that measures database migration timing:

1. **Configuration:**
   - POSTGRES_CONTAINER (default: frostguard-postgres)
   - DB_NAME (default: frostguard)
   - DB_USER (default: frostguard)
   - TEST_DB_NAME (default: frostguard_migration_test)

2. **Pre-flight checks:**
   - Verify Docker is running
   - Verify PostgreSQL container is healthy
   - Verify test data exists (check sensor_readings row count)
   - If < 50,000 rows, warn that test may not be representative

3. **Test flow:**

   Step 1: Record initial row counts for key tables
   - sensor_readings
   - alerts
   - units
   - organizations

   Step 2: Create test database for restore validation
   - CREATE DATABASE frostguard_migration_test

   Step 3: Measure pg_dump export timing
   - Use custom format (-Fc) with compression level 9 (-Z 9)
   - Export to /tmp/migration-timing-test.dump
   - Record duration in seconds
   - Record dump file size

   Step 4: Measure pg_restore import timing
   - Restore to test database
   - Record duration in seconds

   Step 5: Verify data integrity
   - Compare row counts between original and restored
   - Verify counts match for all tables

   Step 6: Cleanup
   - Remove dump file
   - DROP DATABASE frostguard_migration_test

4. **Output:**
   - Summary with timing for each step
   - Total migration window estimate
   - Recommendations for production (e.g., "For 100K records, expect ~X minutes")
   - Scale factor note (e.g., "Production with 1M records: multiply by ~10x")

5. **Error handling:**
   - Graceful cleanup on failure
   - Clear error messages for common issues
     </action>
     <verify>
     Run: chmod +x scripts/test/validate-migration-timing.sh && shellcheck scripts/test/validate-migration-timing.sh
     Script should pass shellcheck validation.
     </verify>
     <done>
     validate-migration-timing.sh measures pg_dump/pg_restore timing.
     Script provides clear migration window estimates.
     Script passes shellcheck validation.
     </done>
     </task>

<task type="auto">
  <name>Task 3: Update README with migration testing documentation</name>
  <files>scripts/test/README.md</files>
  <action>
Append to scripts/test/README.md documentation for migration testing:

1. **Migration Timing Validation:**
   - Purpose: Estimate maintenance window for production migration
   - Prerequisites: Docker running, local stack up

2. **Synthetic Data Generation (generate-test-data.ts):**
   - How to run: npx tsx scripts/test/generate-test-data.ts
   - Configurable parameters (TARGET_RECORDS, BATCH_SIZE)
   - Expected runtime for 100K records
   - Note: Requires backend database connection

3. **Migration Timing (validate-migration-timing.sh):**
   - How to run: ./scripts/test/validate-migration-timing.sh
   - What it measures
   - How to interpret results
   - Scaling guidance (e.g., linear scaling for row count)

4. **Production Migration Planning:**
   - Recommend running timing test with representative data volume
   - Document that pg_dump is single-threaded (per table)
   - Reference Phase 10 backup/restore documentation for production procedures
   - Note: RTO target is 30 minutes, RPO is 24 hours (from Phase 10)

5. **Cleanup:**
   - How to remove test data after validation
   - truncate sensor_readings command (with warning about production)
     </action>
     <verify>
     Run: grep -c "migration" scripts/test/README.md
     Should return at least 5 references to migration-related content.
     </verify>
     <done>
     README documents migration testing workflow.
     Production planning guidance included.
     </done>
     </task>

</tasks>

<verification>
Test the complete migration timing workflow:
```bash
cd /home/skynet/freshtrack-pro-local/freshtrack-pro
# 1. Generate test data (takes ~2-5 minutes)
npx tsx scripts/test/generate-test-data.ts --yes

# 2. Run migration timing validation

./scripts/test/validate-migration-timing.sh

```
</verification>

<success_criteria>
- generate-test-data.ts creates 100K synthetic sensor readings
- validate-migration-timing.sh measures pg_dump/pg_restore timing
- Migration window is documented with clear estimates
- TEST-03 requirement (migration procedure tested) is addressed
</success_criteria>

<output>
After completion, create `.planning/phases/13-e2e-validation-cutover/13-03-SUMMARY.md`
</output>
```
