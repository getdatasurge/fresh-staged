---
phase: 06-data-migration-scripts
plan: 04
type: execute
wave: 3
depends_on: ["06-01", "06-02", "06-03"]
files_modified:
  - scripts/migration/import.ts
  - scripts/migration/lib/import-helpers.ts
autonomous: true

must_haves:
  truths:
    - "Import script loads JSON files into new PostgreSQL"
    - "Tables imported in dependency order (no FK violations)"
    - "User ID columns mapped from Supabase to Stack Auth IDs"
    - "Timestamps preserved exactly as exported"
    - "Import fails fast on any error"
  artifacts:
    - path: "scripts/migration/import.ts"
      provides: "Main import script with transaction support"
      exports: ["main"]
    - path: "scripts/migration/lib/import-helpers.ts"
      provides: "Import utilities with user ID mapping"
      exports: ["importTable", "importTableWithMapping", "truncateAllTables"]
  key_links:
    - from: "scripts/migration/import.ts"
      to: "scripts/migration/lib/new-db-client.ts"
      via: "import"
      pattern: "import.*new-db-client"
    - from: "scripts/migration/import.ts"
      to: "scripts/migration/lib/user-mapping.ts"
      via: "loadMapping"
      pattern: "loadMapping"
    - from: "scripts/migration/lib/import-helpers.ts"
      to: "mapUserId"
      via: "user ID transformation"
      pattern: "mapUserId"
---

<objective>
Create import script to load exported JSON data into the new PostgreSQL database.

Purpose: Import all exported data while applying user ID mapping and respecting foreign key constraints. Fail fast on errors - with an 8+ hour maintenance window, restart from scratch is acceptable.

Output:
- scripts/migration/import.ts - Main import CLI
- scripts/migration/lib/import-helpers.ts - Import utilities
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-data-migration-scripts/06-CONTEXT.md
@.planning/phases/06-data-migration-scripts/06-RESEARCH.md
@.planning/phases/06-data-migration-scripts/06-01-SUMMARY.md
@.planning/phases/06-data-migration-scripts/06-02-SUMMARY.md
@.planning/phases/06-data-migration-scripts/06-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create import helpers with user ID mapping</name>
  <files>
    scripts/migration/lib/import-helpers.ts
  </files>
  <action>
Create import-helpers.ts with utilities for loading data:

1. importTable(pool, tableName, jsonPath):
   - Load JSON file
   - For each row, INSERT into table
   - Use parameterized queries to prevent SQL injection
   - Handle NULL values correctly
   - Return row count
   - Batch inserts (500 rows per transaction) for large tables

2. importTableWithMapping(pool, tableName, jsonPath, userMapping, userIdColumns):
   - Same as importTable but transforms user ID columns
   - For each userIdColumn in the row:
     - Look up new ID using mapUserId(userMapping, oldId)
     - Replace old ID with new ID
     - Log warning if mapping not found (but continue)
   - This is used for tables in TABLES_WITH_USER_IDS

3. truncateAllTables(pool, tables):
   - Truncate tables in REVERSE dependency order
   - Use CASCADE to handle remaining FK constraints
   - Useful for restart-from-scratch strategy
   - Requires confirmation (--yes flag)

4. disableForeignKeys(pool) / enableForeignKeys(pool):
   - SET session_replication_role = 'replica' to disable FK checks during bulk import
   - SET session_replication_role = 'origin' to re-enable
   - Alternative to importing in exact dependency order

5. Handle data type conversions:
   - Parse ISO 8601 timestamps: new Date(value).toISOString()
   - Parse numeric strings back to NUMERIC: just pass as string, PG handles it
   - Parse JSONB: JSON.stringify(value) if object
  </action>
  <verify>
Unit test with mock data: create temp table, import test rows, verify row count and data integrity.
  </verify>
  <done>
Import helpers can load JSON into tables with proper data type handling and user ID mapping.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create main import script with CLI</name>
  <files>
    scripts/migration/import.ts
  </files>
  <action>
Create import.ts as the main import CLI:

1. CLI options with commander:
   - --input-dir (default: ./migration-data)
   - --mapping (default: ./migration-data/user-mapping.json)
   - --table (optional, import single table)
   - --truncate-first (truncate target tables before import, requires --yes)
   - --disable-fk (disable foreign key checks during import)
   - --yes (skip confirmation prompts)
   - --dry-run (validate files exist without importing)

2. Main import flow:
   ```typescript
   async function main() {
     const pool = await getNewDbClient();
     const opts = program.opts();

     // Load user ID mapping
     const userMapping = await loadMapping(opts.mapping);
     logger.info({ mappingCount: userMapping.size }, 'Loaded user ID mapping');

     // Load export metadata
     const metadata = JSON.parse(
       await fs.readFile(path.join(opts.inputDir, 'metadata.json'), 'utf-8')
     );

     // Optional: truncate tables first (for restart)
     if (opts.truncateFirst) {
       if (!opts.yes) {
         const answer = await prompt('This will DELETE all data. Continue? (yes/no): ');
         if (answer !== 'yes') process.exit(1);
       }
       await truncateAllTables(pool, [...TABLE_IMPORT_ORDER].reverse());
     }

     // Optional: disable foreign key checks
     if (opts.disableFk) {
       await disableForeignKeys(pool);
     }

     // Import each table in dependency order
     const tables = getTableImportOrder();
     for (const table of tables) {
       const jsonPath = path.join(opts.inputDir, `${table}.json`);

       if (!fs.existsSync(jsonPath)) {
         logger.warn({ table }, 'JSON file not found, skipping');
         continue;
       }

       logger.info({ table }, 'Importing table...');

       try {
         let rowCount: number;
         if (requiresUserMapping(table)) {
           const userIdColumns = getUserIdColumns(table);
           rowCount = await importTableWithMapping(
             pool, table, jsonPath, userMapping, userIdColumns
           );
         } else {
           rowCount = await importTable(pool, table, jsonPath);
         }

         logger.info({ table, rowCount }, 'Imported');
       } catch (err) {
         logger.error({ err, table }, 'Import FAILED');
         throw err; // Fail fast
       }
     }

     // Re-enable foreign key checks
     if (opts.disableFk) {
       await enableForeignKeys(pool);
     }

     logger.info('Import complete');
   }
   ```

3. Transaction handling:
   - Each table imported in a transaction
   - Rollback on error, fail fast
   - Log which tables succeeded before failure

4. Progress indicator with ora:
   - Show current table and progress
   - Update with row count
  </action>
  <verify>
Run `pnpm exec tsx import.ts --dry-run` to verify file discovery. Test with small dataset if target DB available.
  </verify>
  <done>
Import script loads JSON files into new database with user ID mapping and proper error handling.
  </done>
</task>

</tasks>

<verification>
1. `pnpm exec tsx import.ts --help` shows CLI options
2. `pnpm exec tsx import.ts --dry-run` validates export files exist
3. User ID columns correctly mapped using loaded mapping file
4. Tables imported in dependency order (no FK violations)
5. --truncate-first with --yes clears target tables
6. Fail-fast behavior: error in one table stops entire import
</verification>

<success_criteria>
- Import script runs without FK constraint violations
- User ID mapping applied to all columns in TABLES_WITH_USER_IDS
- Timestamps preserved exactly (no timezone shifts)
- Row counts match exported metadata
- Clear error messages on failure with table name and row context
</success_criteria>

<output>
After completion, create `.planning/phases/06-data-migration-scripts/06-04-SUMMARY.md`
</output>
