---
phase: 06-data-migration-scripts
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - scripts/migration/lib/stream-helpers.ts
  - scripts/migration/export.ts
autonomous: true

must_haves:
  truths:
    - "Export script reads all tables from Supabase"
    - "Data exported as JSON files (one per table)"
    - "Large tables use streaming to avoid memory issues"
    - "Timestamps preserved in ISO 8601 format"
    - "Progress logged to console and file"
  artifacts:
    - path: "scripts/migration/export.ts"
      provides: "Main export script"
      exports: ["main"]
    - path: "scripts/migration/lib/stream-helpers.ts"
      provides: "Streaming utilities for large tables"
      exports: ["streamTableToJson", "exportSmallTable"]
  key_links:
    - from: "scripts/migration/export.ts"
      to: "scripts/migration/lib/supabase-client.ts"
      via: "import"
      pattern: "import.*supabase-client"
    - from: "scripts/migration/export.ts"
      to: "scripts/migration/lib/table-metadata.ts"
      via: "import TABLE_IMPORT_ORDER"
      pattern: "TABLE_IMPORT_ORDER"
---

<objective>
Create export script to extract all data from Supabase PostgreSQL to JSON files.

Purpose: Export all production data from Supabase in a format that preserves data integrity and can be transformed during import. JSON format allows human inspection and debugging.

Output:
- scripts/migration/export.ts - Main export CLI
- scripts/migration/lib/stream-helpers.ts - Streaming utilities
- migration-data/*.json files when executed
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-data-migration-scripts/06-CONTEXT.md
@.planning/phases/06-data-migration-scripts/06-RESEARCH.md
@.planning/phases/06-data-migration-scripts/06-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create streaming helpers for large tables</name>
  <files>
    scripts/migration/lib/stream-helpers.ts
  </files>
  <action>
Create stream-helpers.ts with utilities for efficient data export:

1. streamTableToJson(pool, tableName, outputPath):
   - Use pg-query-stream for memory-efficient streaming
   - Query: SELECT * FROM {tableName} ORDER BY id (or primary key)
   - Convert timestamps to ISO 8601 format with explicit UTC timezone
   - Write JSON array format with streaming (not loading all rows in memory)
   - Log progress every 1000 rows
   - Return row count

2. exportSmallTable(pool, tableName, outputPath):
   - For tables < 10,000 rows (organizations, profiles, sites, etc.)
   - Simple SELECT * with JSON.stringify
   - Handle numeric precision (convert PostgreSQL numeric to string for lossless JSON)
   - Return row count

3. getTableRowCount(pool, tableName):
   - SELECT COUNT(*) FROM tableName
   - Used to decide streaming vs simple export

4. Constants:
   - STREAMING_THRESHOLD = 10000 (rows)
   - LARGE_TABLES = ['sensor_readings', 'event_logs', 'alerts'] (always stream)

Handle edge cases:
- NULL values: Preserve as JSON null
- JSONB columns: Export as native JSON
- Timestamps: Use to_json() or explicit AT TIME ZONE 'UTC'
- Numeric precision: Export numeric columns as strings to avoid float issues
  </action>
  <verify>
Test by exporting a small test table (like organizations if accessible) to a temp file.
  </verify>
  <done>
Stream helpers export tables to JSON with proper data type handling and progress logging.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create main export script with CLI</name>
  <files>
    scripts/migration/export.ts
  </files>
  <action>
Create export.ts as the main export CLI:

1. CLI setup with commander:
   - --output-dir (default: ./migration-data)
   - --table (optional, export single table)
   - --skip-large (skip sensor_readings and event_logs for quick test)
   - --dry-run (just list tables without exporting)

2. Main export flow:
   ```typescript
   async function main() {
     const pool = await getSupabaseClient();
     const outputDir = program.opts().outputDir;

     // Create output directory
     await fs.mkdir(outputDir, { recursive: true });

     // Get tables to export (TABLE_IMPORT_ORDER from metadata)
     const tables = getTableImportOrder();

     // Export metadata first
     const metadata = {
       exportedAt: new Date().toISOString(),
       sourceDatabase: 'supabase',
       tables: {} as Record<string, { rowCount: number }>
     };

     // Export each table
     for (const table of tables) {
       logger.info({ table }, 'Exporting table...');
       try {
         const outputPath = path.join(outputDir, `${table}.json`);
         const rowCount = await getTableRowCount(pool, table);

         if (rowCount > STREAMING_THRESHOLD || LARGE_TABLES.includes(table)) {
           await streamTableToJson(pool, table, outputPath);
         } else {
           await exportSmallTable(pool, table, outputPath);
         }

         metadata.tables[table] = { rowCount };
         logger.info({ table, rowCount }, 'Exported');
       } catch (err) {
         logger.error({ err, table }, 'Export failed');
         throw err; // Fail fast
       }
     }

     // Write metadata
     await fs.writeFile(
       path.join(outputDir, 'metadata.json'),
       JSON.stringify(metadata, null, 2)
     );

     logger.info('Export complete');
   }
   ```

3. Also export auth.users separately:
   - Requires direct PostgreSQL connection (not Supabase SDK)
   - Query: SELECT id, email, encrypted_password, email_confirmed_at, created_at, raw_user_meta_data FROM auth.users
   - Save to auth_users.json (separate from public schema tables)

4. Progress indicator with ora:
   - Show spinner during export
   - Update with current table name and row count
  </action>
  <verify>
Run `pnpm exec tsx export.ts --dry-run` to verify table list. If Supabase connection available, test with --table organizations.
  </verify>
  <done>
Export script lists all tables in dry-run mode. With valid connection, exports tables to JSON files with metadata.
  </done>
</task>

</tasks>

<verification>
1. `pnpm exec tsx export.ts --help` shows CLI options
2. `pnpm exec tsx export.ts --dry-run` lists all tables in correct order
3. Stream helpers handle both small and large tables
4. Timestamps are in ISO 8601 UTC format
5. Numeric values preserved without precision loss (as strings)
6. Progress logged to console and log file
</verification>

<success_criteria>
- Export script runs without errors (at least in dry-run mode)
- Tables exported in dependency order from TABLE_IMPORT_ORDER
- Large tables (sensor_readings) use streaming to avoid OOM
- auth.users exported separately for user ID mapping
- metadata.json contains export timestamp and row counts
</success_criteria>

<output>
After completion, create `.planning/phases/06-data-migration-scripts/06-02-SUMMARY.md`
</output>
